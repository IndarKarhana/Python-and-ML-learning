from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Read the CSV file into a PySpark DataFrame
mi_data = spark.read.csv("..\clfm_testing\manually_identified_outliers2.csv", header=True, inferSchema=True)

# Convert 'Service Point Id' to integer
mi_data = mi_data.withColumn("Service Point Id", mi_data["Service Point Id"].cast("int"))

# Convert 'usage date' to datetime
mi_data = mi_data.withColumn("usage date", F.to_date("usage date"))

# Read the text file into a PySpark DataFrame
sm_data = spark.read.option("delimiter", "\t").csv("..\clfm_testing\SmartMeterDataExport.txt", header=True, inferSchema=True)

# Convert 'usage date' to datetime
sm_data = sm_data.withColumn("usage date", F.to_date("usage date"))

# Completing date range for each Service point ID
window_spec = Window.partitionBy("Service Point Id").orderBy("usage date")
sm_data = sm_data.withColumn("usg", F.col("usg").cast("float")).withColumn("usg", F.when(F.col("usg") == "?", None).otherwise(F.col("usg")))

# Calculate rolling mean
sm_data = sm_data.withColumn("rolling_mean", F.mean("usg").over(window_spec.rowsBetween(-1, 0)))

# Filter rows where 'usg' > rolling_mean
sm_data = sm_data.filter(F.col("usg") > F.col("rolling_mean"))

# Concatenate index and Service Point Id into a new column
sm_data = sm_data.withColumn("key", F.concat_ws("_", F.col("index"), F.col("Service Point Id")))

# Show the result
sm_data.show()


*------*
def detect_outliers_z_score(data: DataFrame, t: float):
    outlier_indices = []
    extreme_indices = []
    threshold = t
    
    # Calculate mean and standard deviation
    stats = data.agg(F.mean("usg"), F.stddev("usg")).first()
    mean = stats[0]
    std = stats[1]
    
    # Calculate Z-score for each row
    z_scores = data.withColumn("Z_score", (F.col("usg") - mean) / std)
    
    # Filter rows based on Z-score and value of 'usg'
    outliers = z_scores.filter((F.abs(F.col("Z_score")) > threshold) | (F.col("usg") < 0))
    
    # Collect outlier indices and extreme outliers
    outliers_list = outliers.select("key").rdd.flatMap(lambda x: x).collect()
    for key in outliers_list:
        if data.filter(data["key"] == key).select((F.col("usg") + 1) / mean > 10).collect()[0][0]:
            extreme_indices.append(key)
    
    return outliers_list, extreme_indices


def detect_outliers_iqr(data: DataFrame, lower_percentile=10, upper_percentile=90, multiplier=1.5):
    outlier_indices = []
    extreme_indices = []
    
    # Calculate quartiles and IQR
    quartiles = data.approxQuantile("usg", [lower_percentile / 100, upper_percentile / 100], 0.01)
    q1 = quartiles[0]
    q3 = quartiles[1]
    iqr = q3 - q1
    
    # Calculate bounds
    lower_bound = q1 - multiplier * iqr
    upper_bound = q3 + multiplier * iqr
    
    # Filter outliers based on bounds and 'usg' value
    outliers = data.filter((F.col("usg") < lower_bound) | (F.col("usg") > upper_bound) | (F.col("usg") < 0))
    outliers_list = outliers.select("key").rdd.flatMap(lambda x: x).collect()
    
    # Identify extreme outliers
    for key in outliers_list:
        if data.filter(data["key"] == key).select((F.col("usg") / F.mean(data["usg"])) > 10).collect()[0][0]:
            extreme_indices.append(key)
    
    return outliers_list, extreme_indices

*----------*
from pyspark.sql import functions as F
from pyspark.sql import DataFrame

def flatten(xss):
    return [x for xs in xss for x in xs]

def detector_z(data: DataFrame, t=1.5):
    outlier_indices_to_flag = []
    extreme_indices_to_flag = []
    data_ = data.dropna()
    uniq_SPid = data_.select("Service Point Id").distinct().rdd.flatMap(lambda x: x).collect()
    
    for id in uniq_SPid:
        data_t = data_.filter(data_["Service Point Id"] == id)
        outlier_indices_values, extreme_indices_values = detect_outliers_z_score(data_t, t)
        outlier_indices_to_flag.append(outlier_indices_values)
        extreme_indices_to_flag.append(extreme_indices_values)
    
    outlier_indices = flatten(outlier_indices_to_flag)
    extreme_indices = flatten(extreme_indices_to_flag)
    
    data_ = data_.withColumn("outlier_", F.col("key").isin(outlier_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    data_ = data_.withColumn("extreme_", F.col("key").isin(extreme_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    
    return data_

def detector_iqr(data: DataFrame, lower_percentile=10, upper_percentile=90, multiplier=1.5):
    outlier_indices_to_flag = []
    extreme_indices_to_flag = []
    data_ = data.dropna()
    uniq_SPid = data_.select("Service Point Id").distinct().rdd.flatMap(lambda x: x).collect()
    
    for id in uniq_SPid:
        data_t = data_.filter(data_["Service Point Id"] == id)
        outlier_indices_values, extreme_indices_values = detect_outliers_iqr(data_t, lower_percentile, upper_percentile, multiplier)
        outlier_indices_to_flag.append(outlier_indices_values)
        extreme_indices_to_flag.append(extreme_indices_values)
    
    outlier_indices = flatten(outlier_indices_to_flag)
    extreme_indices = flatten(extreme_indices_to_flag)
    
    data_ = data_.withColumn("outlier_", F.col("key").isin(outlier_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    data_ = data_.withColumn("extreme_", F.col("key").isin(extreme_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    
    return data_

**-----------------**
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import numpy as np

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Hypertuning") \
    .getOrCreate()

# Define t values
t_vals = np.arange(1, 0, -0.1)
flag = 0
val_t = 0
val_f = 0
t_value = []
false_negatives = []
true_negatives = []
false_positives = []
true_positives = []
positives = []
negatives = []

# Hypertuning loop
for t in t_vals:
    data_ = detector_z(sm_data, t)
    data_ = data_.withColumn("usage date", F.col("usage date").cast("string"))
    data_ = data_.withColumn("Service Point Id", F.col("Service Point Id").cast("long"))
    data3 = mi_data.join(data_, ["Service Point Id", "usage date"], "left")

    t_value.append(t)

    p_p = data3.filter(col("outlier") == "T").count()
    t_p = data3.filter((col("outlier") == col("outlier_")) & (col("outlier") == "T")).count()
    f_p = data3.filter((col("outlier") != col("outlier_")) & (col("outlier") == "F")).count()
    n_n = data3.filter(col("outlier") == "F").count()
    t_n = n_n - f_p
    f_n = p_p - t_p

    positives.append(p_p)
    true_positives.append(t_p)
    false_positives.append(f_p)
    negatives.append(n_n)
    true_negatives.append(t_n)
    false_negatives.append(f_n)

    if (f_p == val_f) & (t_p == val_t):
        flag += 1
    else:
        val_f = f_p
        val_t = t_p
        flag = 0

    if flag == 3:
        break

# Create DataFrame for hypertuned data
hypertuned_data = {"method": ["z_score"] * len(t_value),
                   "t_value": t_value,
                   "positives": positives,
                   "true positives": true_positives,
                   "false_negatives": false_negatives,
                   "negatives": negatives,
                   "true_negatives": true_negatives,
                   "false_positives": false_positives}
hypertuned_df = spark.createDataFrame(pd.DataFrame(hypertuned_data))

# Write DataFrame to CSV
file_name = "hypertuned_data_z_score.csv"
hypertuned_df.write.csv(file_name, mode="overwrite", header=True)

# Show the DataFrame
hypertuned_df.show()

