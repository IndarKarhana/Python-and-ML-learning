from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Read the CSV file into a PySpark DataFrame
mi_data = spark.read.csv("..\clfm_testing\manually_identified_outliers2.csv", header=True, inferSchema=True)

# Convert 'Service Point Id' to integer
mi_data = mi_data.withColumn("Service Point Id", mi_data["Service Point Id"].cast("int"))

# Convert 'usage date' to datetime
mi_data = mi_data.withColumn("usage date", F.to_date("usage date"))

# Read the text file into a PySpark DataFrame
sm_data = spark.read.option("delimiter", "\t").csv("..\clfm_testing\SmartMeterDataExport.txt", header=True, inferSchema=True)

# Convert 'usage date' to datetime
sm_data = sm_data.withColumn("usage date", F.to_date("usage date"))

# Completing date range for each Service point ID
window_spec = Window.partitionBy("Service Point Id").orderBy("usage date")
sm_data = sm_data.withColumn("usg", F.col("usg").cast("float")).withColumn("usg", F.when(F.col("usg") == "?", None).otherwise(F.col("usg")))

# Calculate rolling mean
sm_data = sm_data.withColumn("rolling_mean", F.mean("usg").over(window_spec.rowsBetween(-1, 0)))

# Filter rows where 'usg' > rolling_mean
sm_data = sm_data.filter(F.col("usg") > F.col("rolling_mean"))

# Concatenate index and Service Point Id into a new column
sm_data = sm_data.withColumn("key", F.concat_ws("_", F.col("index"), F.col("Service Point Id")))

# Show the result
sm_data.show()
