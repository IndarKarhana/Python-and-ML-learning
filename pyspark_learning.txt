from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# Read the CSV file into a PySpark DataFrame
mi_data = spark.read.csv("..\clfm_testing\manually_identified_outliers2.csv", header=True, inferSchema=True)

# Convert 'Service Point Id' to integer
mi_data = mi_data.withColumn("Service Point Id", mi_data["Service Point Id"].cast("int"))

# Convert 'usage date' to datetime
mi_data = mi_data.withColumn("usage date", F.to_date("usage date"))

# Read the text file into a PySpark DataFrame
sm_data = spark.read.option("delimiter", "\t").csv("..\clfm_testing\SmartMeterDataExport.txt", header=True, inferSchema=True)

# Convert 'usage date' to datetime
sm_data = sm_data.withColumn("usage date", F.to_date("usage date"))

# Completing date range for each Service point ID
window_spec = Window.partitionBy("Service Point Id").orderBy("usage date")
sm_data = sm_data.withColumn("usg", F.col("usg").cast("float")).withColumn("usg", F.when(F.col("usg") == "?", None).otherwise(F.col("usg")))

# Calculate rolling mean
sm_data = sm_data.withColumn("rolling_mean", F.mean("usg").over(window_spec.rowsBetween(-1, 0)))

# Filter rows where 'usg' > rolling_mean
sm_data = sm_data.filter(F.col("usg") > F.col("rolling_mean"))

# Concatenate index and Service Point Id into a new column
sm_data = sm_data.withColumn("key", F.concat_ws("_", F.col("index"), F.col("Service Point Id")))

# Show the result
sm_data.show()


*------*
def detect_outliers_z_score(data: DataFrame, t: float):
    outlier_indices = []
    extreme_indices = []
    threshold = t
    
    # Calculate mean and standard deviation
    stats = data.agg(F.mean("usg"), F.stddev("usg")).first()
    mean = stats[0]
    std = stats[1]
    
    # Calculate Z-score for each row
    z_scores = data.withColumn("Z_score", (F.col("usg") - mean) / std)
    
    # Filter rows based on Z-score and value of 'usg'
    outliers = z_scores.filter((F.abs(F.col("Z_score")) > threshold) | (F.col("usg") < 0))
    
    # Collect outlier indices and extreme outliers
    outliers_list = outliers.select("key").rdd.flatMap(lambda x: x).collect()
    for key in outliers_list:
        if data.filter(data["key"] == key).select((F.col("usg") + 1) / mean > 10).collect()[0][0]:
            extreme_indices.append(key)
    
    return outliers_list, extreme_indices


def detect_outliers_iqr(data: DataFrame, lower_percentile=10, upper_percentile=90, multiplier=1.5):
    outlier_indices = []
    extreme_indices = []
    
    # Calculate quartiles and IQR
    quartiles = data.approxQuantile("usg", [lower_percentile / 100, upper_percentile / 100], 0.01)
    q1 = quartiles[0]
    q3 = quartiles[1]
    iqr = q3 - q1
    
    # Calculate bounds
    lower_bound = q1 - multiplier * iqr
    upper_bound = q3 + multiplier * iqr
    
    # Filter outliers based on bounds and 'usg' value
    outliers = data.filter((F.col("usg") < lower_bound) | (F.col("usg") > upper_bound) | (F.col("usg") < 0))
    outliers_list = outliers.select("key").rdd.flatMap(lambda x: x).collect()
    
    # Identify extreme outliers
    for key in outliers_list:
        if data.filter(data["key"] == key).select((F.col("usg") / F.mean(data["usg"])) > 10).collect()[0][0]:
            extreme_indices.append(key)
    
    return outliers_list, extreme_indices

*----------*
from pyspark.sql import functions as F
from pyspark.sql import DataFrame

def flatten(xss):
    return [x for xs in xss for x in xs]

def detector_z(data: DataFrame, t=1.5):
    outlier_indices_to_flag = []
    extreme_indices_to_flag = []
    data_ = data.dropna()
    uniq_SPid = data_.select("Service Point Id").distinct().rdd.flatMap(lambda x: x).collect()
    
    for id in uniq_SPid:
        data_t = data_.filter(data_["Service Point Id"] == id)
        outlier_indices_values, extreme_indices_values = detect_outliers_z_score(data_t, t)
        outlier_indices_to_flag.append(outlier_indices_values)
        extreme_indices_to_flag.append(extreme_indices_values)
    
    outlier_indices = flatten(outlier_indices_to_flag)
    extreme_indices = flatten(extreme_indices_to_flag)
    
    data_ = data_.withColumn("outlier_", F.col("key").isin(outlier_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    data_ = data_.withColumn("extreme_", F.col("key").isin(extreme_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    
    return data_

def detector_iqr(data: DataFrame, lower_percentile=10, upper_percentile=90, multiplier=1.5):
    outlier_indices_to_flag = []
    extreme_indices_to_flag = []
    data_ = data.dropna()
    uniq_SPid = data_.select("Service Point Id").distinct().rdd.flatMap(lambda x: x).collect()
    
    for id in uniq_SPid:
        data_t = data_.filter(data_["Service Point Id"] == id)
        outlier_indices_values, extreme_indices_values = detect_outliers_iqr(data_t, lower_percentile, upper_percentile, multiplier)
        outlier_indices_to_flag.append(outlier_indices_values)
        extreme_indices_to_flag.append(extreme_indices_values)
    
    outlier_indices = flatten(outlier_indices_to_flag)
    extreme_indices = flatten(extreme_indices_to_flag)
    
    data_ = data_.withColumn("outlier_", F.col("key").isin(outlier_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    data_ = data_.withColumn("extreme_", F.col("key").isin(extreme_indices).cast("string").replace(['true', 'false'], ['T', 'F']))
    
    return data_


