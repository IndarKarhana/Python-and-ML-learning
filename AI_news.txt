AI Set to Revolutionize Concierge Services - https://news.osu.edu/ai-poised-to-usher-in-new-level-of-concierge-services-to-the-public/
A new paper highlights AI's potential to transform concierge services, enhancing customer interaction in hotels and other sectors. Researchers envision AI concierges using natural language processing, behavioural data, and predictive analytics to anticipate needs, suggest actions, and automate tasks. Despite challenges, AI could offer 24/7 availability, consistency, and tailored experiences, revolutionizing customer service.

AI-generated empathy has its limits - https://news.cornell.edu/stories/2024/05/ai-generated-empathy-has-its-limits
Conversational agents (CAs) like Alexa and Siri, powered by large language models (LLMs), can display empathy but often inherit human biases from their training data. Researchers found that CAs make biased value judgments and struggle with complex empathy compared to humans. Despite potential benefits, mitigating these biases is crucial for ethical AI deployment.

Bio-inspired cameras and AI help drivers detect pedestrians and obstacles faster - https://techxplore.com/news/2024-05-bio-cameras-ai-drivers-pedestrians.html
Researchers developed an AI-based system using a bio-inspired camera, achieving 100 times faster detection of pedestrians and obstacles than current automotive cameras. This hybrid system combines standard and event cameras, improving safety for self-driving cars by detecting objects quickly and using less computational power without compromising accuracy.

Controlled Diffusion Model Alters Material Properties in Images - https://news.mit.edu/2024/controlled-diffusion-model-can-change-material-properties-images-0528
MIT CSAIL and Google Research researchers developed "Alchemist," a diffusion model that alters material properties in images, such as roughness, metallicity, albedo, and transparency. Alchemist uses a slider-based interface for precise control over these attributes, improving photo editing, video game models, visual effects, and robotic training data. Despite some limitations, it offers significant advancements in AI-driven image manipulation.

AI Method Accurately Identifies Actions in Videos - https://news.mit.edu/2024/ai-based-method-can-find-specific-video-action-0529
Researchers developed an AI method to pinpoint specific actions in long instructional videos using only videos and transcripts, avoiding costly hand-labelled data. This model distinguishes spatial and temporal details, enhancing accuracy in identifying actions and human-object interactions, benefiting online learning, virtual training, and healthcare settings.

Researchers analyze the characteristics of AI-generated deep fakes - https://techxplore.com/news/2024-05-characteristics-ai-generated-deepfakes.html
Research shows most AI-generated deepfakes on social media involve political figures and artists, often linked to news cycles. These deepfakes pose significant threats, especially during elections or conflicts. The study recommends media literacy, reverse image searches, and legislation for AI content identification to combat misinformation.

# Convert usg_dt column to datetime type
data['usg_dt'] = pd.to_datetime(data['usg_dt'])

# Get the date 4 months before the last date in the dataset
end_date = data['usg_dt'].max()
start_date = end_date - pd.DateOffset(months=4)

# Filter the data to include only the last 4 months
filtered_data = data[(data['usg_dt'] >= start_date) & (data['usg_dt'] <= end_date)]

# Calculate the 10th and 95th percentiles for usg_dly_bl_tmpp for each sp_id_x
percentiles = filtered_data.groupby('sp_id_x')['usg_dly_bl_tmpp'].quantile([0.10, 0.95]).unstack()

# Filter the data within the 10th to 95th percentile range for each sp_id_x
filtered_data = filtered_data[filtered_data.apply(lambda row: percentiles.loc[row['sp_id_x'], 0.10] <= row['usg_dly_bl_tmpp'] <= percentiles.loc[row['sp_id_x'], 0.95], axis=1)]

# Calculate min and max of usg_dly_bl_tmpp for the filtered data
stats = filtered_data.groupby('sp_id_x')['usg_dly_bl_tmpp'].agg(['min', 'max']).reset_index()
stats.rename(columns={'min': 'min_filtered', 'max': 'max_filtered'}, inplace=True)

# Merge the stats back to the original dataframe
data = data.merge(stats, on='sp_id_x')

# Min-max normalize the usg_n1 column and store it in a new column
data['usg_n1_min_max'] = data.groupby('sp_id_x')['usg_n1'].transform(lambda x: (x - x.min()) / (x.max() - x.min() + 0.0001))

# Denormalize usg_n1_min_max using the calculated min and max from the filtered data
data['usg_n1_denorm'] = data['usg_n1_min_max'] * (data['max_filtered'] - data['min_filtered']) + data['min_filtered']


